% URGENT
Clean up code
Testing 
Conjugate Gradient
presentation slides



get seperate subroutines working
try mlevel for special case


*** PRIORITIES ***
Check equation is symmetric (might need to do steady state)
parallelise code - implement communication within jacobi and conjugate gradient module
set up solver for both conjugate gradient and normal - need to properly set up initial condition
get communication and solution initialisation sorted - for parallelisation
try to implement red-black jacobi solver

% MAIN %
set a constant boundary condition
change boundary conditions
fix residual using allreduce
work on conjugate gradient stuff
plot initial temp distribution
test for longer time period/smaller residual
need to update modules to remove redundancy
update matlab automation script
clean up code
get multiple processors working

Implement local indices
Implement communication
Initialise variables with parallelisation
get it working with simple parallelisation independent of claras code and then implement claras variables
copy maxtest code into main.f90
need to get comms working in cg solver
copy comms into maxtest.f90
check i = il,ih and j = jl,jh arent going outside of matrix bounds
dot product in conjugate gradient


check over looping indices for residual
use new boundary condition


output tecplot from maxtest.f90


Conjugate gradient method
TO DO:
    Update preconditioning solver

QUESTIONS
    Where does the communication occur in the conjugate gradient method


F90 = gfortran

outProg: residuals.f90 jacobi.f90 varmod_MP.f90 maxtest.f90

# Testing pure jacobi
mpif90 varmod_MP.f90 residuals.f90 init.f90 jacobi.f90 maxtest.f90 -o test1.exe


outProg: residuals.f90 jacobi.f90 varmod_MP.f90 CGSolver.f90 maxtest.f90

# Testing CG Solver
gfortran outputmod_MP.f90 varmod_MP.f90 residuals.f90 jacobi.f90 CGSolver.f90 maxtest.f90 -o test1.exe # serial
mpif90 outputmod_MP.f90 varmod_MP.f90 residuals.f90 jacobi.f90 CGSolver.f90 maxtest.f90 -o test1.exe # mpi





# Main compilation
mpif90 outputmod_MP.f90 varmod_MP.f90 nodemod_MP.f90 partitioning_MP.f90 residuals.f90 init.f90 jacobi.f90 main.f90 -o test1.exe

##############################################

 ! Calculation of solution using Conjugate Gradient Method
			allocate(Minv(il:ih,jl:jh))
			allocate(dmat(il:ih,jl:jh))
			allocate(qmat(il:ih,jl:jh))
			allocate(smat(il:ih,jl:jh))

			do j = jl,jh	
				do i = il,ih
					ap(i,j) = 1
					an(i,j) = ((dt*alpha)/(dx*dx))
					as(i,j) = ((dt*alpha)/(dx*dx))
					ae(i,j) = ((dt*alpha)/(dx*dx))
					aw(i,j) = ((dt*alpha)/(dx*dx))
					b(i,j) = (1- (4*dt*alpha)/(dx*dx))

					! ap(i,j) = (1- (4*dt*alpha)/(dx*dx))
					! an(i,j) = ((dt*alpha)/(dx*dx))
					! as(i,j) = ((dt*alpha)/(dx*dx))
					! ae(i,j) = ((dt*alpha)/(dx*dx))
					! aw(i,j) = ((dt*alpha)/(dx*dx))
					! b(i,j) = 0
				end do
			end do

			! Pre-conditioner counter and check
			niter_precon = 5
			precon = .TRUE.

			! Initialising solution constants and vectors
			! Initialising delta and delta0
			delta = 1.0
			delta_o = 1.0
			! Initialising dot product variable
			dp = 0.0
			! Initialising aconst and beta
			aconst = 1.0
			beta = 1.0
			! Initialising q and s
			Minv(:,:) = 0.0
			dmat = 0.0
			qmat(:,:) = 0.0
			smat(:,:) = 0.0

			! Calculating d matrix (search vector matrix)
			do j = jl,jh
			! do j = 1,nx
				do i = il,ih
				! do i = 1,ny
					Minv(i,j) = 1/ap(i,j)
				end do
			end do

			if (precon) then
				do j = jl,jh
					do i = il,ih
						dmat(i,j) = Minv(i,j)*resmat(i,j)
					end do
				end do
			end if

			! initiliasing residuals
			call respar(aw,ae,an,as,ap,b,T,il,ih,jl,jh,resmat)
			
			! Calculate Domain averaged residual for stopping critterion
			! rcurrent = SUM(SUM(ABS(resmat(resil:resih,resjl:resjh)),1),1) / ((resih-resil+1)*(resjh-resjl+1))
			
			rcurrent = SUM(SUM(ABS(resmat(il:ih,jl:jh)),1),1) / ((ih-il+1)*(jh-jl+1))
			! Summing processor residuals to get global resiudal and broadcasting to get average
			call MPI_ALLREDUCE(rcurrent,rc,1,MPI_DOUBLE_PRECISION,MPI_SUM,COMM_TOPO,ierr)
			rc = rc/nprocs

			! Pre-conditioning matrix
			if (precon) then
				! call jacpre(ae,aw,an,as,ap,resmat,Minv,dmat,il,ih,jl,jh)

				 do j = jl+1,jh-1
					do i = il+1,ih-1
						dmat(i,j) = resmat(i,j)*Minv(i,j)
					end do
				end do	

				do ii = 1,niter_precon
				
					! Get Residual of Current system Ax = b
					CALL respar(aw,ae,an,as,ap,resmat,dmat,il,ih,jl,jh,resmat)
					
					! Update Solution
					do j = jl+1,jh-1
						do i = il+1,ih-1
							dmat(i,j) = dmat(i,j) + resmat(i,j)*Minv(i,j)
						end do
					end do
						
				end do
			else
				dmat = resmat
			end if

			!-------------------------------------------------------------------!
			! COMMUNICATION				
			! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
			! Receive from the east1, and put it into the space for the ghost node on the east side
			CALL MPI_IRECV( dmat(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
							east1, tag, COMM_TOPO, request_array(1), ierr)
			! Send to the east1
			CALL MPI_ISEND( dmat(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
							east1, tag, COMM_TOPO, request_array(2), ierr)
							
			! Receive from the east2, and put it into the space for the ghost node on the east side
			CALL MPI_IRECV( dmat(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
							east2, tag, COMM_TOPO, request_array(3), ierr)
			! Send to the east2
			CALL MPI_ISEND( dmat(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
							east2, tag, COMM_TOPO, request_array(4), ierr)


			! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
			! Receive from the west1, put it into the space for the ghost node on the west side 
			CALL MPI_IRECV( dmat(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
							west1, tag, COMM_TOPO, request_array(5), ierr)
			! Send to the west1
			CALL MPI_ISEND( dmat(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
							west1, tag, COMM_TOPO, request_array(6), ierr)
							
			! Receive from the west2, put it into the space for the ghost node on the west side 
			CALL MPI_IRECV( dmat(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
							west2, tag, COMM_TOPO, request_array(7), ierr)
			! Send to the west2
			CALL MPI_ISEND( dmat(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
							west2, tag, COMM_TOPO, request_array(8), ierr)
							
			! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
			! Receive from the north, put it into the space for the ghost node 
			CALL MPI_IRECV( dmat(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
							north, tag, COMM_TOPO, request_array(9), ierr)
			! Send to the north
			CALL MPI_ISEND( dmat(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
							north, tag, COMM_TOPO, request_array(10), ierr)
			! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
			! Receive from the south, put it into the space for the ghost node 
			CALL MPI_IRECV( dmat(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
							south, tag, COMM_TOPO, request_array(11), ierr)
			! Send to the south
			CALL MPI_ISEND( dmat(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
							south, tag, COMM_TOPO, request_array(12), ierr)

			! Wait for data sends to complete before black points start referencing red points
			CALL MPI_WAITALL(12, request_array, status_array, ierr)


		! write(*,*) pid, "matrix after solver"
		! call printmatrix(dmat, SIZE(T, DIM=1), SIZE(T, DIM=2))

			! Calculating delta and delta0
			do j = jl+1,jh-1
				do i = il+1,ih-1
					dp = dp + resmat(i,j)*dmat(i,j)
				end do
			end do

			! Assigning values for delta and delta_o
			delta = dp
			delta_o = delta
			
			!-------------------------------------------------------------------------!
			!-------------------------------------------------------------------------!
			!-------------------------------------------------------------------------!
			! Begin solution loop
			do while ((rc>res_max).and.(time<t_final))

				! Compute q matrix
				do j = jl+1,jh-1
					do i = il+1,ih-1
						qmat(i,j) = aw(i,j)*dmat(i-1,j) + ae(i,j)*dmat(i+1,j) + an(i,j)*dmat(i,j+1) + as(i,j)*dmat(i,j-1) &
								+ ap(i,j)*dmat(i,j)
					end do
				end do

				!-------------------------------------------------------------------!
				! COMMUNICATION				
				! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
				! Receive from the east1, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( qmat(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(1), ierr)
				! Send to the east1
				CALL MPI_ISEND( qmat(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(2), ierr)
								
				! Receive from the east2, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( qmat(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(3), ierr)
				! Send to the east2
				CALL MPI_ISEND( qmat(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(4), ierr)


				! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
				! Receive from the west1, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( qmat(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
								west1, tag, COMM_TOPO, request_array(5), ierr)
				! Send to the west1
				CALL MPI_ISEND( qmat(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
								west1, tag, COMM_TOPO, request_array(6), ierr)
								
				! Receive from the west2, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( qmat(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
								west2, tag, COMM_TOPO, request_array(7), ierr)
				! Send to the west2
				CALL MPI_ISEND( qmat(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
								west2, tag, COMM_TOPO, request_array(8), ierr)
								
				! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
				! Receive from the north, put it into the space for the ghost node 
				CALL MPI_IRECV( qmat(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(9), ierr)
				! Send to the north
				CALL MPI_ISEND( qmat(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(10), ierr)
				! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
				! Receive from the south, put it into the space for the ghost node 
				CALL MPI_IRECV( qmat(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(11), ierr)
				! Send to the south
				CALL MPI_ISEND( qmat(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(12), ierr)

				! Wait for data sends to complete before black points start referencing red points
				CALL MPI_WAITALL(12, request_array, status_array, ierr)

				dp = 0
				! compute aconst - note computing dot product in the numerator
				do j = jl+1,jh-1
					do i = il+1,ih-1
						dp = dp + dmat(i,j)*qmat(i,j)
					end do
				end do
				aconst = delta/dp

				! updating T
				do j = jl+1,jh-1
					do i = il+1,ih-1
						T(i,j) = T(i,j) + aconst*dmat(i,j)
					end do
				end do

				!-------------------------------------------------------------------!
				! COMMUNICATION				
				! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
				! Receive from the east1, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( T(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(1), ierr)
				! Send to the east1
				CALL MPI_ISEND( T(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(2), ierr)
								
				! Receive from the east2, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( T(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(3), ierr)
				! Send to the east2
				CALL MPI_ISEND( T(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(4), ierr)


				! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
				! Receive from the west1, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( T(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
								west1, tag, COMM_TOPO, request_array(5), ierr)
				! Send to the west1
				CALL MPI_ISEND( T(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
								west1, tag, COMM_TOPO, request_array(6), ierr)
								
				! Receive from the west2, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( T(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
								west2, tag, COMM_TOPO, request_array(7), ierr)
				! Send to the west2
				CALL MPI_ISEND( T(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
								west2, tag, COMM_TOPO, request_array(8), ierr)
								
				! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
				! Receive from the north, put it into the space for the ghost node 
				CALL MPI_IRECV( T(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(9), ierr)
				! Send to the north
				CALL MPI_ISEND( T(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(10), ierr)
				! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
				! Receive from the south, put it into the space for the ghost node 
				CALL MPI_IRECV( T(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(11), ierr)
				! Send to the south
				CALL MPI_ISEND( T(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(12), ierr)

				! Wait for data sends to complete before black points start referencing red points
				CALL MPI_WAITALL(12, request_array, status_array, ierr)


				! Updating residual
				if ((MOD(iter,50).eq.0)) then
					call respar(aw,ae,an,as,ap,b,T,il,ih,jl,jh,resmat)
				else
					do j = jl+1,jh-1
						do i = il+1,ih-1
							resmat(i,j) = resmat(i,j) - aconst*qmat(i,j)
						end do
					end do
				end if

				!-------------------------------------------------------------------!
				! COMMUNICATION				
				! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
				! Receive from the east1, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( resmat(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(1), ierr)
				! Send to the east1
				CALL MPI_ISEND( resmat(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(2), ierr)
								
				! Receive from the east2, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( resmat(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(3), ierr)
				! Send to the east2
				CALL MPI_ISEND( resmat(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(4), ierr)


				! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
				! Receive from the west1, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( resmat(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
								west1, tag, COMM_TOPO, request_array(5), ierr)
				! Send to the west1
				CALL MPI_ISEND( resmat(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
								west1, tag, COMM_TOPO, request_array(6), ierr)
								
				! Receive from the west2, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( resmat(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
								west2, tag, COMM_TOPO, request_array(7), ierr)
				! Send to the west2
				CALL MPI_ISEND( resmat(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
								west2, tag, COMM_TOPO, request_array(8), ierr)
								
				! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
				! Receive from the north, put it into the space for the ghost node 
				CALL MPI_IRECV( resmat(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(9), ierr)
				! Send to the north
				CALL MPI_ISEND( resmat(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(10), ierr)
				! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
				! Receive from the south, put it into the space for the ghost node 
				CALL MPI_IRECV( resmat(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(11), ierr)
				! Send to the south
				CALL MPI_ISEND( resmat(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(12), ierr)

				! Wait for data sends to complete before black points start referencing red points
				CALL MPI_WAITALL(12, request_array, status_array, ierr)

				! Pre-conditioning s matrix
				if (precon) then
					! call jacpre(ae,aw,an,as,ap,resmat,Minv,smat,il,ih,jl,jh)
					

					 do j = jl+1,jh-1
						do i = il+1,ih-1
							smat(i,j) = resmat(i,j)*Minv(i,j)
						end do
					end do	

					do ii = 1,niter_precon
					
						! Get Residual of Current system Ax = b
						CALL respar(aw,ae,an,as,ap,resmat,smat,resil,resih,resjl,resjh,resmat)
						! Update Solution
						do j = jl+1,jh-1
							do i = il+1,ih-1
								smat(i,j) = smat(i,j) + resmat(i,j)*Minv(i,j)
							end do
						end do
							
					end do
				else
					smat = resmat
				end if

				!-------------------------------------------------------------------!
				! COMMUNICATION				
				! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
				! Receive from the east1, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( smat(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(1), ierr)
				! Send to the east1
				CALL MPI_ISEND( smat(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(2), ierr)
								
				! Receive from the east2, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( smat(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(3), ierr)
				! Send to the east2
				CALL MPI_ISEND( smat(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(4), ierr)


				! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
				! Receive from the west1, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( smat(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
								west1, tag, COMM_TOPO, request_array(5), ierr)
				! Send to the west1
				CALL MPI_ISEND( smat(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
								west1, tag, COMM_TOPO, request_array(6), ierr)
								
				! Receive from the west2, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( smat(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
								west2, tag, COMM_TOPO, request_array(7), ierr)
				! Send to the west2
				CALL MPI_ISEND( smat(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
								west2, tag, COMM_TOPO, request_array(8), ierr)
								
				! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
				! Receive from the north, put it into the space for the ghost node 
				CALL MPI_IRECV( smat(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(9), ierr)
				! Send to the north
				CALL MPI_ISEND( smat(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(10), ierr)
				! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
				! Receive from the south, put it into the space for the ghost node 
				CALL MPI_IRECV( smat(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(11), ierr)
				! Send to the south
				CALL MPI_ISEND( smat(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(12), ierr)

				! Wait for data sends to complete before black points start referencing red points
				CALL MPI_WAITALL(12, request_array, status_array, ierr)

				delta_o = delta

				dp = 0
				! Calculating delta and delta0
				do j = jl+1,jh-1
					do i = il+1,ih-1
						dp = dp + resmat(j,i)*smat(i,j)
					end do
				end do
				
				delta = dp

				! Computing beta
				beta = delta/delta_o
				
				! Updating search vector
				do j = jl+1,jh-1
					do i = il+1,ih-1
						dmat(i,j) = smat(i,j) + beta*dmat(i,j)
					end do
				end do

				!-------------------------------------------------------------------!
				! COMMUNICATION				
				! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
				! Receive from the east1, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( dmat(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(1), ierr)
				! Send to the east1
				CALL MPI_ISEND( dmat(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(2), ierr)
								
				! Receive from the east2, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( dmat(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(3), ierr)
				! Send to the east2
				CALL MPI_ISEND( dmat(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(4), ierr)


				! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
				! Receive from the west1, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( dmat(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
								west1, tag, COMM_TOPO, request_array(5), ierr)
				! Send to the west1
				CALL MPI_ISEND( dmat(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
								west1, tag, COMM_TOPO, request_array(6), ierr)
								
				! Receive from the west2, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( dmat(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
								west2, tag, COMM_TOPO, request_array(7), ierr)
				! Send to the west2
				CALL MPI_ISEND( dmat(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
								west2, tag, COMM_TOPO, request_array(8), ierr)
								
				! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
				! Receive from the north, put it into the space for the ghost node 
				CALL MPI_IRECV( dmat(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(9), ierr)
				! Send to the north
				CALL MPI_ISEND( dmat(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(10), ierr)
				! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
				! Receive from the south, put it into the space for the ghost node 
				CALL MPI_IRECV( dmat(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(11), ierr)
				! Send to the south
				CALL MPI_ISEND( dmat(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(12), ierr)

				! Wait for data sends to complete before black points start referencing red points
				CALL MPI_WAITALL(12, request_array, status_array, ierr)


				! Update residual check	
				! rcurrent = SUM(sum(ABS(Told(resil:resih,resjl:resjh) - T(resil:resih,resjl:resjh)),1),1) &
				! 					/ ((resih-resil+1)*(resjh-resjl+1))
				! rcurrent = SUM(SUM(ABS(resmat(resil:resih,resjl:resjh)),1),1) / ((resih-resil+1)*(resjh-resjl+1))

				rcurrent = SUM(SUM(ABS(resmat(il:ih,jl:jh)),1),1) / ((ih-il+1)*(jh-jl+1))
				! ! Summing processor residuals to get global resiudal and broadcasting to get average
				! call MPI_ALLREDUCE(rcurrent,rc,1,MPI_DOUBLE_PRECISION,MPI_SUM,COMM_TOPO,ierr)
				! rc = rc/nprocs	
				rc = rcurrent

				Told = T

				!-------------------------------------------------------------------!
				! Printing to screen after a certain amount of time
				! if ((time-int(time))<dt) then
				if ((MOD(iter,100).eq.0)) then
					! TECPLOT

					! --------------------------------------------------------------------------------------- 
					!     _____     ____
					!    /      \  |  o | 
					!   |        |/ ___\|                TECPLOT
					!   |_________/     
					!   |_|_| |_|_|
					!	  
					! ---------------------------------------------------------------------------------------

					! These subarrays are now sent to PID 0
					CALL MPI_ISEND( T, 1, SENDINGSUBBARAY, 0, tag2, COMM_TOPO, request_array_gather(1), ierr)	
							
					! Pid 0 receiving data and putting into final file
					IF (pid .EQ. 0) THEN

						DO i = 0, Nprocs-1
						
							! Creating receive subarray type bespoke to each processor
							CALL MPI_Type_create_subarray(2, [ny, nx], [subarray_rows_array(i+1), subarray_cols_array(i+1)], &
							[subarray_row_start(i+1),subarray_col_start(i+1)], MPI_ORDER_FORTRAN, MPI_DOUBLE_PRECISION, RECVSUBBARAY, ierr)
							
							CALL MPI_TYPE_COMMIT(RECVSUBBARAY, ierr)
						
							! Receiving with this new receiving subarray type
							CALL MPI_IRECV( Tfinal, 1, RECVSUBBARAY, &
								i, tag2, COMM_TOPO, request_array_gather(i+2), ierr)
						END DO

						CALL MPI_WAITALL(Nprocs+1, request_array_gather, status_array_gather, ierr)
						
						! --- PUTTING INTO FILE ---
						! x vector of whole domain (could have also done a mpi_gatherv)
						xtot 	= 0. + dx * [(i, i=0,(nx-1))] ! Implied DO loop used
						! y vector of whole domain (could have also done a mpi_gatherv)
						ytot 	= 0. + dy * [(i, i=0,(ny-1))] ! Implied DO loop used

						! Writing updated solution to file at each new iteration
						write(file_name, "(A9,I5,A4)") "TecPlot2D",int(time),".tec"
						CALL tecplot_2D ( iunit, nx, ny, xtot, ytot, Tfinal,  file_name )
				
						write(*,*) '-----------------------------------------'
						write(*,*) 'Time =', time
						write(*,*) 'Iteration =', iter
						write(*,*) 'Residual =', rc
						write(*,*) 'Average Temperature =', sum(sum((Tfinal),1),1)/(nx*ny)
						write(*,*) '-----------------------------------------'
						
					end if
					
				end if

				! Updating timer and iteration counter
				time = time + dt
				iter = iter + 1

			end do 



##############################################

!-------------------------------------------------------------------------------------------!
    !-------------------------------------------------------------------------------------------!
    !-------------------------------------------------------------------------------------------!
    !-------------------------------------------------------------------------------------------!
    !-------------------------------------------------------------------------------------------!
    ! jacobi preconditioner for conjugate gradient
    subroutine jacpre(ae,aw,an,as,ap,b,Minv,T,il,ih,jl,jh)

        IMPLICIT NONE
        Real(kind = 8), INTENT(IN), dimension(il:ih,jl:jh) :: ae, aw, an, as, ap, b, Minv
        Real(kind = 8), INTENT(INOUT), dimension(il:ih,jl:jh) :: T
        Integer(kind = 8), INTENT(IN) :: il,ih,jl,jh

        Integer(kind = 8) :: i,j,ii, niter_precon

        ! Setting number of preconditioning iterations
        niter_precon = 5
        
        do j = jl+1,jh-1
            do i = il+1,ih-1
                T(i,j) = b(i,j)*Minv(i,j)
            end do
        end do	

        do ii = 1,niter_precon
        
            ! Get Residual of Current system Ax = b
            CALL respar(aw,ae,an,as,ap,b,T,il,ih,jl,jh,resmat)
            
            ! Update Solution
            do j = jl+1,jh-1
                do i = il+1,ih-1
                    T(i,j) = T(i,j) + resmat(i,j)*Minv(i,j)
                end do
            end do
                
	    end do

    end subroutine

				



subroutine residcalc(aw,ae,an,as,ap,b,x,res)
        IMPLICIT NONE

        ! Initialising variables
        real(kind=8), dimension(nx,ny), INTENT(IN) :: aw,ae,an,as,ap,b,x
        real(kind=8), dimension(nx,ny), INTENT(OUT) :: res
        
        INTEGER(kind = 8) :: i,j
        
        do j = 2,(ny-1)
        ! do j = jl,jh
            do i = 2,(nx-1)
            ! do i = il,ih           
                res(i,j) = b(i,j) - ( as(i,j)*x(i-1,j) + an(i,j)*x(i+1,j) + ae(i,j)*x(i,j+1) &
                                                    + aw(i,j)*x(i,j-1) + ap(i,j)*x(i,j) )		           
            end do
        end do		

    end subroutine residcalc


	! if (pid .eq. 0) then
	! 	write(*,*) pid, "before comms"
	! 	call printmatrix(T, SIZE(T, DIM=1), SIZE(T, DIM=2))
	! end if
	! if (pid .eq. 1) then
	! 	write(*,*) pid, "before comms"
	! 	call printmatrix(T, SIZE(T, DIM=1), SIZE(T, DIM=2))
	! end if
	

	! SOLVER OUTLINE
		! outer loop: time stepping 
			! solve for time step n+1 and while r<err
				! conjugate gradient/jacobi/redback

				! this will involve looping through temperature arrays
				! end solve for timestep n+1

				! update resduals
			
			! If statement check that t=somevalue
			! WRITE TO ONE FILE 
			
		! end time stepping
		
		! Initialisation module
		! Residual module
		! Solver module - jacobi/red-black/Conjugate Gradient
		! Call communication

! ! JUST DOING MY OWN TECPLOT FILE WRITING
    ! ! Creating a temp array to combine all processor data
    ! ALLOCATE(Ttemp(resil:resih,resjl:resjh))

	! if (pid==0) then
		! allocate(Tinittot(nx,ny))
	! end if

    ! ! RUNNING A CHECK TO SEE IF INTIIAL TEMP IS BEING CALCULATED CORRECTLY
    ! Ttemp = T(resil:resih,resjl:resjh)

	! npp = nx/nprocs
    ! ! Gathering data to processor 1
    ! CALL MPI_GATHER(Ttemp,ny*npp,MPI_DOUBLE_PRECISION,Tinittot,ny*npp,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ierr)

	! allocate(x(jl:jh))
	! allocate(y(il:ih))

	! ! x is in the j direction, y is in the i direction
	! do i = il,ih
		! y(i) = (i-1)*dy
	! end do
	! do j = jl,jh
		! x(j) = (j-1)*dx
	! end do

	! if (pid==0) then
		! write(*,1600) Tinittot

		! ! Writing updated initial distribution to file
        ! write(file_name, "(A14)") "Tecplotmax.tec"
        ! call tecplot_2D ( iunit, nx, ny, x, y, Tinittot, file_name )
	! end if

	! ! Computing residual matrix
	! call respar(aw,ae,an,as,ap,b,T,resil,resih,resjl,resjh,resmat)

	! ! Calculate Domain averaged residual for stopping critterion
	! rcurrent = SUM(SUM(ABS(resmat(resil:resih,resjl:resjh)),1),1) / ((resih-resil+1)*(resjh-resjl+1))
	! ! write(*,*) rcurrent, rc

	! ! ! Combining all residuals on each processor and sending to processor 0
	! call MPI_REDUCE(rcurrent,rc,1,MPI_DOUBLE_PRECISION,MPI_SUM,0,MPI_COMM_WORLD,ierr)

	! ! Getting global averaged residual
	! if (pid == 0) then
	! 	rc = rc/nprocs
	! end if

	! ! Sending out global residual to each processor
	! call MPI_BCAST(rc,1,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ierr)


        if (time == 0) then
            do j = jl+1,jh-1
                do i = il+1,ih-1
                    Tn(i,j) = (T(i+1,j)*an(i,j) + T(i-1,j)*as(i,j) + T(i,j+1)*ae(i,j) &
                        + T(i,j-1)*aw(i,j) + T(i,j)*ap(i,j))/2
                end do
            end do
        else
            do j = jl+1,jh-1
                do i = il+1,ih-1
                    Tn(i,j) = T(i+1,j)*an(i,j) + T(i-1,j)*as(i,j) + T(i,j+1)*ae(i,j) &
                        + T(i,j-1)*aw(i,j) + T(i,j)*ap(i,j) - Told(i,j)
                end do
            end do
        end if




 !-------------------------------------------------------------------------------------------!
    !-------------------------------------------------------------------------------------------!
    !-------------------------------------------------------------------------------------------!
    !-------------------------------------------------------------------------------------------!
    !-------------------------------------------------------------------------------------------!
    ! subroutine to solve jacobi - completely serial atm
    SUBROUTINE jacobisolv(an,as,ae,aw,ap,b,x,y,T)

        IMPLICIT NONE
        ! Getting variables from varmod module
            ! nx - number of x points
            ! ny - number of y points
            ! dx - discretisation in x
            ! dy - discretisation in y
            ! dt - temporal discretisation
            ! dt - temporal discretisation
            ! alpha - thermal diffusivity
            ! t_final - total time

        ! Defining variables
        REAL(kind=8), INTENT(IN) :: an(nx,ny), as(nx,ny), ae(nx,ny), aw(nx,ny), ap(nx,ny), b(nx,ny), x(nx), y(ny)
        REAL(kind=8), INTENT(INOUT) :: T(nx,ny)

        Real(kind = 8) :: time, rcurrent
        Real(kind = 8) :: Tn(nx,ny), Told(nx,ny), res(nx,ny)
        Real(kind = 8) :: Minv(nx,ny)
        Integer :: i,j,iter, ii, il,jl,ih,jh

        ! Set code case for type of solver, norm - for jacobi solving, redblack - for redblack jacobi solver
        solving = "norm"

        !------------------------------------------------------------------------------------!
        !------------------------------------------------------------------------------------!

        ! Inititialising old temperature array
        Told(:,:) = 0

        ! Initialising time counter
        time = 0
        iter = 0

 
        ! Iteratively solving the jacobi equation
        ! Solving unsteady 2D Heat diffusion - dT/dt = alpha*(d^2T/dx^2 + d^2T/dy^2)

        SELECT CASE (solving)

            !ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo!
            !ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo!
            !ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo!
            ! Jacobi solver
            CASE ("norm")
       
                ! do while ((t<ttot).and.(res>rmax))
                do while (time<t_final)
                    if (time == 0) then
                        do j = 2,(ny-1)
                        ! do j = jl,jh
                            do i = 2,(nx-1)
                            ! do i = il,ih
                                Tn(i,j) = (T(i+1,j)*ae(i,j) + T(i-1,j)*aw(i,j) + T(i,j+1)*an(i,j) &
                                    + T(i,j-1)*as(i,j) + T(i,j)*ap(i,j))/2
                            end do
                        end do
                    else
                        do j = 2,(ny-1)
                        ! do j = jl,jh
                            do i = 2,(nx-1)
                            ! do i = il,ih
                                Tn(i,j) = T(i+1,j)*ae(i,j) + T(i-1,j)*aw(i,j) + T(i,j+1)*an(i,j) &
                                    + T(i,j-1)*as(i,j) + T(i,j)*ap(i,j) - Told(i,j)
                            end do
                        end do
                    end if


                    ! COMMUNICATION


                    ! computing residuals
                    call residcalc(aw,ae,an,as,ap,b,T,res)

                    ! Calculate Domain averaged residual for stopping critterion
                    ! rcurrent = SUM(SUM(ABS(r(il:ih,jl:jh)),1),1) / ((kx-2)*(ky-2))
                    rcurrent = SUM(SUM(ABS(res(1:nx,1:ny)),1),1) / ((nx-2)*(ny-2))

                    if (mod(iter,100).eq.0) then
                        write(*,*) '      iter', '      res'
                        write(*,*) iter, rcurrent
                    end if

                    ! updating old and new temperature values
                    Told = T
                    T = Tn

                    ! updating counter
                    time = time + dt
                    iter = iter + 1

                end do

            !ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo!
            !ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo!
            !ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo!
            ! red are odd nodes
            ! black are even nodes
            CASE ("redblack")

                ! do while ((t<ttot).and.(res>rmax))
                do while (time<t_final)
                    if (time == 0) then

                        ! RED NODES CALCULATION
                        do j = 2,(ny-1),2
                        ! do j = jl,jh,2
                            do i = 2,(nx-1),2
                            ! do i = il,ih,2
                                Tn(i,j) = (T(i+1,j)*ae(i,j) + T(i-1,j)*aw(i,j) + T(i,j+1)*an(i,j) &
                                    + T(i,j-1)*as(i,j) + T(i,j)*ap(i,j))/2
                            end do
                        end do

                        ! RED NODES COMMUNICATION



                        !---------------------------------------------------------------!
                        !---------------------------------------------------------------!
                        ! BLACK NODES CALCULATION
                        do j = 3,(ny-1),2
                        ! do j = jl+1,jh,2
                            do i = 3,(nx-1),2
                            ! do i = il+1,ih,2
                                Tn(i,j) = (T(i+1,j)*ae(i,j) + T(i-1,j)*aw(i,j) + T(i,j+1)*an(i,j) &
                                    + T(i,j-1)*as(i,j) + T(i,j)*ap(i,j))/2
                            end do
                        end do

                        ! BLACK NODES COMMUNICATION
                    else

                        ! RED NODES CALCULATION
                        do j = 2,(ny-1),2
                        ! do j = jl,jh,2
                            do i = 2,(nx-1),2
                            ! do i = il,ih,2
                                Tn(i,j) = T(i+1,j)*ae(i,j) + T(i-1,j)*aw(i,j) + T(i,j+1)*an(i,j) &
                                    + T(i,j-1)*as(i,j) + T(i,j)*ap(i,j) - Told(i,j)
                            end do
                        end do

                        ! RED NODES COMMUNICATION



                        !---------------------------------------------------------------!
                        !---------------------------------------------------------------!
                        ! BLACK NODES CALCULATION
                        do j = 3,(ny-1),2
                        ! do j = jl+1,jh,2
                            do i = 3,(nx-1),2
                            ! do i = il+1,ih,2
                                Tn(i,j) = T(i+1,j)*ae(i,j) + T(i-1,j)*aw(i,j) + T(i,j+1)*an(i,j) &
                                    + T(i,j-1)*as(i,j) + T(i,j)*ap(i,j) - Told(i,j)
                            end do
                        end do
                        
                        ! BLACK NODES COMMUNICATION

                    end if


                    ! computing residuals
                    call residcalc(aw,ae,an,as,ap,b,T,res)

                    ! Calculate Domain averaged residual for stopping critterion
                    ! rcurrent = SUM(SUM(ABS(r(il:ih,jl:jh)),1),1) / ((kx-2)*(ky-2))
                    rcurrent = SUM(SUM(ABS(res(1:nx,1:ny)),1),1) / ((nx-2)*(ny-2))

                    if (mod(iter,100).eq.0) then
                        write(*,*) '      iter', '      res'
                        write(*,*) iter, rcurrent
                    end if

                    ! updating old and new temperature values
                    Told = T
                    T = Tn

                    ! updating counter
                    time = time + dt
                    iter = iter + 1

                end do

            CASE DEFAULT 
                WRITE(*,*) "No solver selected or incorrect selection"
                STOP
	    END SELECT

        1600 FORMAT(5(F14.8,1x))

    END SUBROUTINE jacobisolv




    ! MAJOR PROJECT MAIN CODE OUTLINE
PROGRAM MAIN
! CALLING MODULES
	USE variablemodule 		! Contains variable allocation all problem-specific variable values
	USE nodemodule			! Obtains nodes and indices of the domain considered by each processor
	USE partitionmodule		! Contains subroutines to partition domain and obtain domain info for each processor
	USE outputmodule		! Creates tec file
	USE cjgradient
	USE jacobi
	USE residuals
	USE solinit

	include 'mpif.h'

! INITIALISE VARIABLES 
	! Variable declaration and assignment
	CALL intialise()

! -------------------------------------------------------------------------------	
!| CODE CASES										───▄▀▀▀▄▄▄▄▄▄▄▀▀▀▄───		 |
	topology = "slabs" 	!							───█▒▒░░░░░░░░░▒▒█───		 |
!|		Options: "graph" "cart" "slabs" 			────█░░█░░░░░█░░█────		 |
    solvertype = "redblack"	!							─▄▄──█░░░▀█▀░░░█──▄▄─		 |
!| 		Options: "jac", "redblack", "conj"			█░░█─▀▄░░░░░░░▄▀─█░░█		 |
! -------------------------------------------------------------------------------

! 	INITIALISE MPI
	CALL MPI_INIT(ierr)
	CALL MPI_COMM_RANK(MPI_COMM_WORLD, pid, ierr) ! Getting processor ID number
	CALL MPI_COMM_SIZE(MPI_COMM_WORLD, Nprocs, ierr) ! Getting number of total processors in global communicator

! Begin timer
t1 = MPI_WTIME()

! INITIALISE PARTITIONING
	! module
	! outputs - local x, local y, low + high indxx, low + high indxy, low + high nodex, low + high nodey, all neighbour pid vals, idx east1/east2/west1/west2, number of send/recv points toe neighbours (east1/east2/west1/west2) 
	
	SELECT CASE (topology)
	
		CASE ("graph")
		
			CALL graph_partition()
			
			! Not reordering because in this configuration MPI doesn't appear to want to reorder them anyway.
			! (I ran the code with reorder=true and compared before/after for different numbers of processors)
			! So setting false to ensure future code still works on the /off/ chance it did reorder them.
			CALL MPI_GRAPH_CREATE(MPI_COMM_WORLD, Nprocs, indexes, edges, .false., COMM_TOPO, ierr)

			! MPI_Graph_neighbors_count retrieves the number of neighbours for a given rank in the communicator
			CALL MPI_GRAPH_NEIGHBORS_COUNT(COMM_TOPO, pid, neighbours_count, ierr)
			WRITE(*,*) 'My pid is', pid, '. I have', neighbours_count, 'neighbours.'
			! MPI_Graph_neighbors - Returns the neighbors of a node associated with a graph topology.
			
			ALLOCATE( neighbours_array(neighbours_count) )
			CALL MPI_GRAPH_NEIGHBORS(COMM_TOPO, pid, neighbours_count, neighbours_array, ierr)
			write(*,*) 'i am pid', pid, 'my neighbours are', neighbours_array
			
		CASE ("cart")
		
			CALL cart_partition()
		
			! Create a new cartesian communicator based on the above analysis
			CALL MPI_CART_CREATE(MPI_COMM_WORLD, ndims, dims, periods, .true., COMM_TOPO, ierr)
			CALL MPI_COMM_RANK(COMM_TOPO, pid, ierr) ! New processor ID number
			CALL MPI_CART_COORDS(COMM_TOPO, pid, ndims, coords, ierr)
			! coords is an array with pid location in (x, y) but y counts from top down and counts start from zero
			! Send/Recv id's for each processor horizontally and vertically (if applicable)
			CALL MPI_CART_SHIFT(COMM_TOPO, 0, 1, west1, east1, ierr)
			CALL MPI_CART_SHIFT(COMM_TOPO, 1, 1, north, south, ierr)
			
			CALL cart_nodesindices()
			
		CASE ("slabs")
		
			CALL slab_partition()
		
			! Create a new cartesian communicator based on the above analysis
			CALL MPI_CART_CREATE(MPI_COMM_WORLD, ndims, dims, periods, .true., COMM_TOPO, ierr)
			CALL MPI_COMM_RANK(COMM_TOPO, pid, ierr) ! New processor ID number
			CALL MPI_CART_COORDS(COMM_TOPO, pid, ndims, coords, ierr)
			! coords is an array with pid location in (x, y) but y counts from top down and counts start from zero
			! Send/Recv id's for each processor horizontally and vertically (if applicable)
			CALL MPI_CART_SHIFT(COMM_TOPO, 0, 1, west1, east1, ierr)
			CALL MPI_CART_SHIFT(COMM_TOPO, 1, 1, north, south, ierr)
			
			CALL slab_nodesindices()
	
		CASE DEFAULT 
		  WRITE(*,*) "No topology selected or incorrect selection"
		  STOP
	END SELECT
	
	! New vector type for sending and receiving the rows north and south (data not contiguous along a row)
	CALL MPI_TYPE_VECTOR(ncalcpoints_x, 1, ncalcpoints_y+2, MPI_DOUBLE_PRECISION, NS_ROW_SENDRECV , ierr)
	CALL MPI_TYPE_COMMIT(NS_ROW_SENDRECV, ierr)
	

! ! !-----------------------------------------------------------------------------------------------------!
! ! !-----------------------------------------------------------------------------------------------------!
		! ┈┈╱▔▔▔▔▔▔▔▏ 
		! ┈╱ ╭▏╮╭┻┻╮╭┻┻╮╭▏ 
		! ▕╮╰▏╯┃╭╮┃┃╭╮┃╰▏ 
		! ▕╯┈▏┈┗┻┻┛┗┻┻┻╮▏ 
		! ▕╭╮▏╮┈┈┈┈┏━━━╯▏
		! ▕╰╯▏╯╰┳┳┳┳┳┳╯╭▏ 
		! ▕┈╭▏╭╮┃┗┛┗┛┃┈╰▏ 
		! ▕┈╰▏╰╯╰━━━━╯┈┈▏

	! ! !-----------------------------------------------------------------------------------------------------!
	! ! !-----------------------------------------------------------------------------------------------------!
	! INITIALISATION

	! Indices for computation
	il = ind_low_y; ih = ind_high_y; jl = ind_low_x; jh = ind_high_x ! Indices for temperature calculations
	resil = node_low_y; resih = node_high_y; resjl = node_low_x; resjh = node_high_x ! Nodes for residual calculations

	!!!!!!!!!!!!!!!!!!!!!!!!!!!
	! INITIALISE TEMP DISTRIBUTION 
	! boundary conditions most pizza like
	! Allocation of variable sizes
	call allocatevars(an,as,ae,aw,ap,b,T,Told,Tn,res,il,ih,jl,jh)

	!!!!!!!!!!!!!!!!!!!!!!!!!!!
	! Initialising boundary conditions on temp array
	call solutioninit(an,as,ae,aw,ap,b,T,il,ih,jl,jh)

	! !!!!!!!!!!!!!!!!!!!!!!!!!!!
	! Initialising new temp array and setting its value to T
	Tn(:,:) = 0
	Tn = T

	!!!!!!!!!!!!!!!!!!!!!!!!!!!
	! INITIAL RESIDUAL
	! Initialising residual matrix
	allocate(resmat(resil:resih,resjl:resjh))
	resmat(:,:) = 0.0
	rc = 1

	!!!!!!!!!!!!!!!!!!!!!!!!!!!
	! STABILITY CRITERION CHECK
	CFL = (1/(dx**2) + 1/(dy**2))*alpha*dt
	if (pid.eq.0) then
		write(*,*) 'Stability criterion: ', CFL
	end if
	if (CFL.GT.0.5) then
		write(*,*) 'Stability criterion not met, needs to be less than 0.5'
		STOP
	elseif (CFL.LT.0) then
		write(*,*) 'Stability criterion less than 0'
		STOP
	end if

	!!!!!!!!!!!!!!!!!!!!!!!!!!!
	! Initialising time counter and iteration counter
	time = 0
	iter = 0

	! ! !-----------------------------------------------------------------------------------------------------!
	! ! !-----------------------------------------------------------------------------------------------------!
	!!!!!!!!!!!!!!!!!!!!!!!!!!! COMPUTATION !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	! ░▄▀▀▀▀▄░░▄▄
	! █░░░░░░▀▀░░█░░░░░░▄░▄
	! █░║░░░░██░████████████ 
	! █░░░░░░▄▄░░█░░░░░░▀░▀
	! ░▀▄▄▄▄▀░░▀▀


	ALLOCATE( status_array(MPI_STATUS_SIZE, 12) )
	ALLOCATE( request_array(12) )
	ALLOCATE( status_array_gather(MPI_STATUS_SIZE, Nprocs+1) )
	ALLOCATE( request_array_gather(Nprocs+1) )

	! All processors first create subarrays. These are the final temperature array cleansed of the extra ghost nodes
	subarray_Nrows = node_high_y-node_low_y + 1
	subarray_Ncols = node_high_x-node_low_x + 1
	
	CALL MPI_Type_create_subarray(2, [ncalcpoints_y+2,ncalcpoints_x+2], [subarray_Nrows,subarray_Ncols], &
	[node_low_y-ind_low_y, node_low_x-ind_low_x], MPI_ORDER_FORTRAN, MPI_DOUBLE_PRECISION, SENDINGSUBBARAY, ierr)
	CALL MPI_TYPE_COMMIT(SENDINGSUBBARAY, ierr)
	
	! If pid 0, then make space to obtain relevant information about each subarray, and the final temp array for the domain 
	IF (pid .EQ. 0) THEN
		ALLOCATE( Tfinal(1:ny,1:nx) )
		ALLOCATE( subarray_rows_array(1:Nprocs) )
		ALLOCATE( subarray_cols_array(1:Nprocs) )
		ALLOCATE( subarray_row_start(1:Nprocs) )
		ALLOCATE( subarray_col_start(1:Nprocs) )
	END IF
	
	! ---- RECIEVE SUBARRAY TYPE ----
	! Gather subarray information from all processors
		! Subarray row sizes
		CALL MPI_GATHER( subarray_Nrows, 1, MPI_INTEGER, subarray_rows_array, 1, MPI_INTEGER, 0, COMM_TOPO, ierr )
		! Subarray column sizes
		CALL MPI_GATHER( subarray_Ncols, 1, MPI_INTEGER, subarray_cols_array, 1, MPI_INTEGER, 0, COMM_TOPO, ierr )
		! Start row location in the final matrix (minus 1 because start location starts at zero)
		CALL MPI_GATHER( node_low_y-1, 	 1, MPI_INTEGER, subarray_row_start,  1, MPI_INTEGER, 0, COMM_TOPO, ierr )
		! Start column location in the final matrix (minus 1 because start location starts at zero)
		CALL MPI_GATHER( node_low_x-1, 	 1, MPI_INTEGER, subarray_col_start,  1, MPI_INTEGER, 0, COMM_TOPO, ierr )
			
	! SOLVING

    SELECT CASE (solvertype)

        ! jacobi solver
        CASE ("jac")
            ! Begin the solution loop
            do while ((time<t_final).and.(rc>res_max))

				!-------------------------------------------------------------------!
                ! Calculation of solution using only jacobi solver				
                call jac(an,as,ae,aw,ap,b,T,il,ih,jl,jh)

				!-------------------------------------------------------------------!
				! COMMUNICATION				
				! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
				! Receive from the east1, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( T(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(1), ierr)
				! Send to the east1
				CALL MPI_ISEND( T(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(2), ierr)
								
				! Receive from the east2, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( T(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(3), ierr)
				! Send to the east2
				CALL MPI_ISEND( T(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(4), ierr)


				! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
				! Receive from the west1, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( T(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
								west1, tag, COMM_TOPO, request_array(5), ierr)
				! Send to the west1
				CALL MPI_ISEND( T(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
								west1, tag, COMM_TOPO, request_array(6), ierr)
								
				! Receive from the west2, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( T(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
								west2, tag, COMM_TOPO, request_array(7), ierr)
				! Send to the west2
				CALL MPI_ISEND( T(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
								west2, tag, COMM_TOPO, request_array(8), ierr)
								
				! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
				! Receive from the north, put it into the space for the ghost node 
				CALL MPI_IRECV( T(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(9), ierr)
				! Send to the north
				CALL MPI_ISEND( T(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(10), ierr)
				! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
				! Receive from the south, put it into the space for the ghost node 
				CALL MPI_IRECV( T(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(11), ierr)
				! Send to the south
				CALL MPI_ISEND( T(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(12), ierr)

				! Wait for data sends to complete before black points start referencing red points
				CALL MPI_WAITALL(12, request_array, status_array, ierr)

				!-------------------------------------------------------------------!
				! RESIDUALS
                ! computing residuals
                call respar(aw,ae,an,as,ap,b,T,resil,resih,resjl,resjh,resmat)

				! Calculate Domain averaged residual for stopping critterion
				rcurrent = SUM(SUM(ABS(resmat(resil:resih,resjl:resjh)),1),1) / ((resih-resil+1)*(resjh-resjl+1))

				! Summing processor residuals to get global resiudal and broadcasting to get average
				call MPI_ALLREDUCE(rcurrent,rc,1,MPI_DOUBLE_PRECISION,MPI_SUM,COMM_TOPO,ierr)
				rc = rc/nprocs

				!-------------------------------------------------------------------!
				! Printing to screen after a certain amount of time
				if ((time-int(time))<dt) then

					! TECPLOT

					! --------------------------------------------------------------------------------------- 
					!     _____     ____
					!    /      \  |  o | 
					!   |        |/ ___\|                TECPLOT
					!   |_________/     
					!   |_|_| |_|_|
					!	  
					! ---------------------------------------------------------------------------------------

					! These subarrays are now sent to PID 0
					CALL MPI_ISEND( T, 1, SENDINGSUBBARAY, 0, tag2, COMM_TOPO, request_array_gather(1), ierr)	
							
					! Pid 0 receiving data and putting into final file
					IF (pid .EQ. 0) THEN

						DO i = 0, Nprocs-1
						
							! Creating receive subarray type bespoke to each processor
							CALL MPI_Type_create_subarray(2, [ny, nx], [subarray_rows_array(i+1), subarray_cols_array(i+1)], &
							[subarray_row_start(i+1),subarray_col_start(i+1)], MPI_ORDER_FORTRAN, MPI_DOUBLE_PRECISION, RECVSUBBARAY, ierr)
							
							CALL MPI_TYPE_COMMIT(RECVSUBBARAY, ierr)
						
							! Receiving with this new receiving subarray type
							CALL MPI_IRECV( Tfinal, 1, RECVSUBBARAY, &
								i, tag2, COMM_TOPO, request_array_gather(i+2), ierr)
						END DO

						CALL MPI_WAITALL(Nprocs+1, request_array_gather, status_array_gather, ierr)
												
						! --- PUTTING INTO FILE ---
						! x vector of whole domain (could have also done a mpi_gatherv)
						xtot 	= 0. + dx * [(i, i=0,(nx-1))] ! Implied DO loop used
						! y vector of whole domain (could have also done a mpi_gatherv)
						ytot 	= 0. + dy * [(i, i=0,(ny-1))] ! Implied DO loop used

						! Writing updated solution to file at each new iteration
						write(file_name, "(A9,I5,A4)") "TecPlot2D",int(time),".tec"
						CALL tecplot_2D ( iunit, nx, ny, xtot, ytot, Tfinal,  file_name )

						write(*,*) '-----------------------------------------'
						write(*,*) 'Time =', time
						write(*,*) 'Iteration =', iter
						write(*,*) 'Residual =', rc
						write(*,*) 'Average Temperature =', sum(sum((Tfinal),1),1)/(nx*ny)
						write(*,*) '-----------------------------------------'
						
					END IF
					
                end if

				!-------------------------------------------------------------------!
                ! updating counter
                time = time + dt
                iter = iter + 1

            end do


! 			 ;               ,           
!          ,;                 '.         
!         ;:                   :;        
!        ::                     ::       
!        ::                     ::       
!        ':                     :        
!         :.                    :        
!      ;' ::                   ::  '     
!     .'  ';                   ;'  '.    
!    ::    :;                 ;:    ::   
!    ;      :;.             ,;:     ::   
!    :;      :;:           ,;"      ::   
!    ::.      ':;  ..,.;  ;:'     ,.;:   
!     "'"...   '::,::::: ;:   .;.;""'    
!         '"""....;:::::;,;.;"""         
!     .:::.....'"':::::::'",...;::::;.   
!    ;:' '""'"";.,;:::::;.'""""""  ':;   
!   ::'         ;::;:::;::..         :;  
!  ::         ,;:::::::::::;:..       :: 
!  ;'     ,;;:;::::::::::::::;";..    ':.
! ::     ;:"  ::::::"""'::::::  ":     ::
!  :.    ::   ::::::;  :::::::   :     ; 
!   ;    ::   :::::::  :::::::   :    ;  
!    '   ::   ::::::....:::::'  ,:   '   
!     '  ::    :::::::::::::"   ::       
!        ::     ':::::::::"'    ::       
!        ':       """""""'      ::       
!         ::                   ;:        
!         ':;                 ;:"        
!           ';              ,;'          
!             "'           '"            
!               '

        CASE ("redblack")

            ! Begin the solution loop
            do while ((time<t_final).and.(rc>res_max))

				!-------------------------------------------------------------------!
                ! calculation of red nodes
                call rednodes(an,as,ae,aw,ap,b,T,il,ih,jl,jh)

				!-------------------------------------------------------------------!
                ! COMMUNICATION of red nodes			
				! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
				! Receive from the east1, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( T(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(1), ierr)
				! Send to the east1
				CALL MPI_ISEND( T(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(2), ierr)
								
				! Receive from the east2, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( T(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(3), ierr)
				! Send to the east2
				CALL MPI_ISEND( T(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(4), ierr)


				! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
				! Receive from the west1, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( T(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
								west1, tag, COMM_TOPO, request_array(5), ierr)
				! Send to the west1
				CALL MPI_ISEND( T(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
								west1, tag, COMM_TOPO, request_array(6), ierr)
								
				! Receive from the west2, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( T(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
								west2, tag, COMM_TOPO, request_array(7), ierr)
				! Send to the west2
				CALL MPI_ISEND( T(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
								west2, tag, COMM_TOPO, request_array(8), ierr)
								
				! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
				! Receive from the north, put it into the space for the ghost node 
				CALL MPI_IRECV( T(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(9), ierr)
				! Send to the north
				CALL MPI_ISEND( T(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(10), ierr)
				! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
				! Receive from the south, put it into the space for the ghost node 
				CALL MPI_IRECV( T(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(11), ierr)
				! Send to the south
				CALL MPI_ISEND( T(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(12), ierr)

				! Wait for data sends to complete before black points start referencing red points
				CALL MPI_WAITALL(12, request_array, status_array, ierr)


				!-------------------------------------------------------------------!
                ! calculation of black nodes
                call blacknodes(an,as,ae,aw,ap,b,T,il,ih,jl,jh)


				!-------------------------------------------------------------------!
                ! COMMUNICATION of black nodes
            	! ------- RECEIVE FROM THE RIGHT/EAST, SEND TO THE RIGHT/EAST
				! Receive from the east1, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( T(ind_low_east1:ind_high_east1, ind_high_x), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(1), ierr)
				! Send to the east1
				CALL MPI_ISEND( T(ind_low_east1:ind_high_east1, ind_high_x-1), ncalcpoints_y_east1, MPI_DOUBLE_PRECISION, &
								east1, tag, COMM_TOPO, request_array(2), ierr)
								
				! Receive from the east2, and put it into the space for the ghost node on the east side
				CALL MPI_IRECV( T(ind_low_east2:ind_high_east2, ind_high_x), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(3), ierr)
				! Send to the east2
				CALL MPI_ISEND( T(ind_low_east2:ind_high_east2, ind_high_x-1), ncalcpoints_y_east2, MPI_DOUBLE_PRECISION, &
								east2, tag, COMM_TOPO, request_array(4), ierr)


				! ------- RECEIVE FROM THE LEFT/WEST, SEND TO THE LEFT/WEST
				! Receive from the west1, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( T(ind_low_west1:ind_high_west1, ind_low_x), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, & 
								west1, tag, COMM_TOPO, request_array(5), ierr)
				! Send to the west1
				CALL MPI_ISEND( T(ind_low_west1:ind_high_west1, ind_low_x+1), ncalcpoints_y_west1, MPI_DOUBLE_PRECISION, &
								west1, tag, COMM_TOPO, request_array(6), ierr)
								
				! Receive from the west2, put it into the space for the ghost node on the west side 
				CALL MPI_IRECV( T(ind_low_west2:ind_high_west2, ind_low_x), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, & 
								west2, tag, COMM_TOPO, request_array(7), ierr)
				! Send to the west2
				CALL MPI_ISEND( T(ind_low_west2:ind_high_west2, ind_low_x+1), ncalcpoints_y_west2, MPI_DOUBLE_PRECISION, &
								west2, tag, COMM_TOPO, request_array(8), ierr)
								
				! ------- RECEIVE FROM THE NORTH, SEND TO THE NORTH
				! Receive from the north, put it into the space for the ghost node 
				CALL MPI_IRECV( T(ind_low_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(9), ierr)
				! Send to the north
				CALL MPI_ISEND( T(ind_low_y+1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								north, tag, COMM_TOPO, request_array(10), ierr)
				! ------- RECEIVE FROM THE SOUTH, SEND TO THE SOUTH
				! Receive from the south, put it into the space for the ghost node 
				CALL MPI_IRECV( T(ind_high_y, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(11), ierr)
				! Send to the south
				CALL MPI_ISEND( T(ind_high_y-1, ind_low_x+1), 1, NS_ROW_SENDRECV, &
								south, tag, COMM_TOPO, request_array(12), ierr)

				! Wait for data sends to complete before black points start referencing red points
				CALL MPI_WAITALL(12, request_array, status_array, ierr)

				!-------------------------------------------------------------------!
				! RESIDUALS
                ! computing residuals
                call respar(aw,ae,an,as,ap,b,T,resil,resih,resjl,resjh,resmat)

				! Calculate Domain averaged residual for stopping critterion
				rcurrent = SUM(SUM(ABS(resmat(resil:resih,resjl:resjh)),1),1) / ((resih-resil+1)*(resjh-resjl+1))

				! Summing processor residuals to get global resiudal and broadcasting to get average
				call MPI_ALLREDUCE(rcurrent,rc,1,MPI_DOUBLE_PRECISION,MPI_SUM,COMM_TOPO,ierr)
				rc = rc/nprocs

				!-------------------------------------------------------------------!
				! Printing to screen after a certain amount of time
				if ((time-int(time))<dt) then
					! TECPLOT

					! --------------------------------------------------------------------------------------- 
					!     _____     ____
					!    /      \  |  o | 
					!   |        |/ ___\|                TECPLOT
					!   |_________/     
					!   |_|_| |_|_|
					!	  
					! ---------------------------------------------------------------------------------------

					! These subarrays are now sent to PID 0
					CALL MPI_ISEND( T, 1, SENDINGSUBBARAY, 0, tag2, COMM_TOPO, request_array_gather(1), ierr)	
							
					! Pid 0 receiving data and putting into final file
					IF (pid .EQ. 0) THEN

						DO i = 0, Nprocs-1
						
							! Creating receive subarray type bespoke to each processor
							CALL MPI_Type_create_subarray(2, [ny, nx], [subarray_rows_array(i+1), subarray_cols_array(i+1)], &
							[subarray_row_start(i+1),subarray_col_start(i+1)], MPI_ORDER_FORTRAN, MPI_DOUBLE_PRECISION, RECVSUBBARAY, ierr)
							
							CALL MPI_TYPE_COMMIT(RECVSUBBARAY, ierr)
						
							! Receiving with this new receiving subarray type
							CALL MPI_IRECV( Tfinal, 1, RECVSUBBARAY, &
								i, tag2, COMM_TOPO, request_array_gather(i+2), ierr)
						END DO

						CALL MPI_WAITALL(Nprocs+1, request_array_gather, status_array_gather, ierr)
						
						
						! --- PUTTING INTO FILE ---
						! x vector of whole domain (could have also done a mpi_gatherv)
						xtot 	= 0. + dx * [(i, i=0,(nx-1))] ! Implied DO loop used
						! y vector of whole domain (could have also done a mpi_gatherv)
						ytot 	= 0. + dy * [(i, i=0,(ny-1))] ! Implied DO loop used

						! Writing updated solution to file at each new iteration
						write(file_name, "(A9,I5,A4)") "TecPlot2D",iter,".tec"
						CALL tecplot_2D ( iunit, nx, ny, xtot, ytot, Tfinal,  file_name )
					
						write(*,*) '-----------------------------------------'
						write(*,*) 'Time =', time
						write(*,*) 'Iteration =', iter
						write(*,*) 'Residual =', rc
						write(*,*) 'Average Temperature =', sum(sum((Tfinal),1),1)/(nx*ny)
						write(*,*) '-----------------------------------------'
						
					END IF
					
                end if


                ! updating counter
                time = time + dt
                iter = iter + 1

            end do

        CASE("Conj")

            ! ! Calculation of solution using Conjugate Gradient Method
            ! ! call CGSolve(an,as,ae,aw,ap,b,T)

    	CASE DEFAULT 
		WRITE(*,*) "No solver selected or incorrect selection"
		STOP

    END SELECT


	if (pid.eq.0) then
		write(*,*) 'Output after solver'
		! write(*,1600) Tfinal
		write(*,*) '-----------------------------------------'
		write(*,*) 'Time =', time
		write(*,*) 'Iteration =', iter
		write(*,*) 'Residual =', rc
		write(*,*) 'CFL =', CFL
		t2 = MPI_WTIME()
		write(*,*) 'Computational time =', t2-t1, 'seconds'
		write(*,*) 'Average Temperature =', sum(sum((Tfinal),1),1)/(nx*ny)
		write(*,*) '-----------------------------------------'

		! Writing the final iteration to a tecplot file
		write(file_name, "(A9,I5,A4)") "TecPlot2D",int(time),".tec"
		CALL tecplot_2D ( iunit, nx, ny, xtot, ytot, Tfinal,  file_name )

		! 	! Checking what size the A matrix coefficients are
		! 	write(*,*) 'A matrix coefficients'
		! 	! write(*,*) an(1,1), as(1,1), ae(1,1), aw(1,1), ap(1,1)
		! 	write(*,*) 'an =', an(1,1)
		! 	write(*,*) 'as =', as(1,1)
		! 	write(*,*) 'ae =', ae(1,1)
		! 	write(*,*) 'aw =', aw(1,1)
		! 	write(*,*) 'ap =', ap(1,1)
		! 	write(*,*) 'b =', b(1,1)

		! write(*,*) pid, "matrix after solver"
		! call printmatrix(Tfinal, SIZE(T, DIM=1), SIZE(T, DIM=2))
	end if
	! Outputting the domain indices
	! write(*,*) nx, ny
	! write(*,*) pid,il,ih,jl,jh
	! write(*,*) pid, resil,resih,resjl,resjh


	
	
	
	! *************************************************
	! --- Collating Final Temperature Arrays ---
	! T arrays from node values need to be sent to processor zero who needs to put them in the right place.
	! Sending must consider that ghost nodes need to be ignored, and that looks different for all processors
	! Receving will also need to consider where the matrix goes in the final domain (will not be contiguous)
	
	! ---- MAXIMUM AND AVERAGE TEMPERATURE ----
	! Printing maximum and average temperature to 8 decimal places:
	
	! ! Max temperature. Can take max over ghost nodes too because it doesn't matter - after global max.
	! CALL MPI_REDUCE(maxval(T), &
	! T_max, 1, MPI_DOUBLE_PRECISION, MPI_MAX, 0, COMM_TOPO, ierr)
	
	! ! Average temperature. Only calculate average over nodes considered by each processor. Ignore ghost nodes.
	! CALL MPI_REDUCE( (1/(DBLE(nx)*DBLE(ny))) * sum( T(node_low_y:node_high_y, node_low_x:node_high_x) ), &
 	! T_avg, 1, MPI_DOUBLE_PRECISION, MPI_SUM, 0, COMM_TOPO, ierr)
	
	! IF (pid .EQ. 0) THEN 
		! WRITE(*,'(A35, f10.8, A16)') "Maximum temperature across domain: ", T_max, " degrees Celcius"
		! WRITE(*,'(A35, f10.8, A16)') "Average temperature across domain: ", T_avg, " degrees Celcius"
	! END IF
	
	! ---- SEND DATA TYPE CREATION ----
	! The final send/recvs will be dependent on number of processors unlike before. Re-allocate relevant arrays to reflect this

CALL MPI_FINALIZE(ierr)

    ! NOTE: Need to update this to match the number of spatial divisions (nx)
    1100 FORMAT(8(F20.10,1x))
    1600 FORMAT(8(F14.8,1x))
    1400 FORMAT(8(F14.8,1x))
    1200 FORMAT(I2.1,I2.1,6(F8.4,1x))
    1300 FORMAT(I2.1,6(F10.8,1x))

END PROGRAM MAIN

! b - matrix
subroutine printmatrix(b,n,m)
	integer::n,m
	real (kind=8)::b(n,m) !n = # rows, m = # columns
	do i=1,n; print '(20f6.2)',b(i,1:m); enddo
endsubroutine




JACOBI stuff

                ! EXPLICIT
                ! UNSTEADY
                ! Tn(i,j) = T(i+1,j)*an(i,j) + T(i-1,j)*as(i,j) + T(i,j+1)*ae(i,j) &
                !     + T(i,j-1)*aw(i,j) + T(i,j)*ap(i,j)
                ! Tn(i,j) = T(i,j)*ap(i,j) + alpha*dt*((T(i,j+1)+T(i,j-1))/(dx**2)) + alpha*dt*((T(i+1,j)+T(i-1,j))/(dy**2))  

                ! STEADY
                ! Tn(i,j) = (alpha*((T(i,j+1)+T(i,j-1))/(dx**2)) & 
                !             + alpha*((T(i+1,j)+T(i-1,j))/(dy**2)))/((2*alpha)/dx**2 + (2*alpha)/dy**2)    

                ! IMPLICIT
                ! JACOBI
                ! bmat(i,j) = T(i,j)/C
                ! bmat(i,j) = (T(i,j)*dx*dx)/(dx*dx + 4*dt*alpha)

                ! bmat(i,j) = T(i,j)/(1D0 + 4D0*C)
                ! Tn(i,j) = bmat(i,j) + F*(Tn(i+1,j) + Tn(i-1,j) + Tn(i,j+1) + Tn(i,j-1))

                ! Tn(i,j) = (1- (4*dt*alpha)/(dx*dx))*T(i,j) + ((dt*alpha)/(dx*dx))*(Tn(i+1,j) + Tn(i-1,j) + Tn(i,j+1) + Tn(i,j-1))


				       ! CRANK NICHOLSON
                ! bmat(i,j) = (1 - (2*alpha*k)/hsq)*T(i,j) + (alpha*k)/(2*hsq)*(T(i+1,j) + T(i-1,j) + T(i,j+1) + T(i,j-1))
                ! Tn(i,j) = (bmat(i,j) + (alpha*k)/(2*hsq)*(Tn(i+1,j) + Tn(i-1,j) + Tn(i,j+1) + Tn(i,j-1)))/(1 + (2*alpha*k)/hsq)



Real(kind = 8) :: C, F, hsq, k
        Real(kind = 8), dimension(il:ih,jl:jh) :: bmat, Temp

        bmat(:,:) = 0_8

        C = ((dt*alpha)/(dx*dx))
        ! F = C/(1D0 + 4D0*C)
        F = (dt*alpha)/(dx*dx + 4*dt*alpha)
        ! hsq = dx**2
        ! k = dt

        ! write(*,*) rc, C, F

                	! if (pid==0) then
					! 	write(*,*) rc, 'during jac'
					! 	call printmatrix(T, SIZE(T, DIM=1), SIZE(T, DIM=2))
					! 	! Stop
                    !     call sleep(1)
					! end if

					! if (pid==1) then
					! 	write(*,*) rc, pid, 'during jac'
					! 	call printmatrix(T, SIZE(T, DIM=1), SIZE(T, DIM=2))
					! 	! Stop
                    !     call sleep(1)
					! end if

                    ! if (pid==0) then
					! 	write(*,*) rc, pid, 'during jac Tn'
					! 	call printmatrix(Tn, SIZE(T, DIM=1), SIZE(T, DIM=2))
					! 	! Stop
                    !     call sleep(1)
					! end if


        			! if (pid==0) then
					! 	write(*,*) rc, 'b matrix'
					! 	call printmatrix(bmat, SIZE(T, DIM=1), SIZE(T, DIM=2))
					! 	! Stop
                    !     call sleep(1)
					! end if

					! if (pid==1) then
					! 	write(*,*) rc, pid, 'b matrix'
					! 	call printmatrix(bmat, SIZE(T, DIM=1), SIZE(T, DIM=2))
					! 	! Stop
                    !     call sleep(1)
					! end if



											! write(*,*) pid, "matrix after solver"
						! call printmatrix(T, SIZE(T, DIM=1), SIZE(T, DIM=2))
						! call sleep(1)
						! stop
												
						! write(*,*) 'after solver'
						! call printmatrix(Tfinal, SIZE(Tfinal, DIM=1), SIZE(Tfinal, DIM=2))
						! ! Stop
						! call sleep(1)